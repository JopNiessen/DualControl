{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Actor-Critic\n",
    "Implementation using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import jax\n",
    "from jax.lax import stop_gradient\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from src.systems.linear import StochasticDoubleIntegrator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1128]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, dim, eta=1e-2):\n",
    "        (n_input, n_hidden, n_out) = dim\n",
    "        self.model = nn.Sequential(nn.Linear(n_input, n_hidden),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(n_hidden, n_out))\n",
    "        self.eta = eta\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=eta)\n",
    "\n",
    "input = torch.tensor([[0.1, 1.0, 0.5]])\n",
    "dim = (3, 32, 1)\n",
    "Net = NeuralNet(dim)\n",
    "x_train = torch.randn(1, 3)\n",
    "Net.model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftQFunction(NeuralNet):\n",
    "    def __init__(self, dimensions, eta=1e-2):\n",
    "        super().__init__(dimensions, eta=eta)\n",
    "        self.gamma = .9\n",
    "        self.sample_size = 1\n",
    "        self.n_epochs = 10\n",
    "    \n",
    "    def loss(self, D, value_func):\n",
    "        bellman_residual = torch.tensor([[0]])\n",
    "        N = len(D)\n",
    "        for it in range(min(N, self.sample_size)):\n",
    "            if it == 0:\n",
    "                s0, u, rew, s1 = D[-1]\n",
    "            else:\n",
    "                idx = np.random.randint(0, N)\n",
    "                s0, u, rew, s1 = D[idx]\n",
    "            Q = self.get_output(s0, u)\n",
    "            Q_hat = rew + self.gamma * value_func(s1)\n",
    "            bellman_residual = bellman_residual + (Q - Q_hat)**2 / 2\n",
    "        return bellman_residual / min(N, self.sample_size)\n",
    "    \n",
    "    def update(self, D, value_func):\n",
    "        losses = []\n",
    "        for _ in range(self.n_epochs):\n",
    "            loss = self.loss(D, value_func)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "        return losses\n",
    "    \n",
    "    def get_output(self, state, control):\n",
    "        input = torch.cat((state, control), axis=1).to(torch.float32)\n",
    "        y_hat = self.model(input)\n",
    "        return y_hat\n",
    "    \n",
    "    def get_value(self, state, control):\n",
    "        return self.get_output(state, control).detach().numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftValueFunction(NeuralNet):\n",
    "    def __init__(self, dimensions, eta=1e-2):\n",
    "        super().__init__(dimensions, eta=eta)\n",
    "        self.sample_size = 1\n",
    "        self.n_epochs = 1\n",
    "    \n",
    "    def loss(self, D, q_func, pi_log_func):\n",
    "        squared_residual_error = 0\n",
    "        N = len(D)\n",
    "        for it in range(min(N, self.sample_size)):\n",
    "            if it == 0:\n",
    "                s0, u, _, _ = D[-1]\n",
    "            else:\n",
    "                idx = np.random.randint(0, N)\n",
    "                s0, u, _, _ = D[idx]\n",
    "            V = self.model(s0)\n",
    "            # Sample u from policy pi\n",
    "            Q = q_func(s0, u)\n",
    "            log_pi = pi_log_func(s0, u)\n",
    "            squared_residual_error += (V - (Q - log_pi))**2 / 2\n",
    "        return squared_residual_error / min(N, self.sample_size)\n",
    "    \n",
    "    def update(self, D, q_func, pi_log_func):\n",
    "        losses = []\n",
    "        for _ in range(self.n_epochs):\n",
    "            loss = self.loss(D, q_func, pi_log_func)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "        return losses\n",
    "    \n",
    "    def get_value(self, input):\n",
    "        return self.model(input).detach().numpy()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4414/1225164696.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_s0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_s1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mSQF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mSVF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSQF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms0_estimate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSQF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4414/4178749481.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, D, value_func)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "class SoftPolicyFunction:\n",
    "    def __init__(self, dimensions, eta=1e-2):\n",
    "        (n_input, n_out) = dimensions\n",
    "        self.model = nn.Sequential(nn.Linear(n_input, n_out))\n",
    "        self.eta = eta\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=eta)\n",
    "        self.stdev = .5\n",
    "    \n",
    "    def loss(self, D, q_func):\n",
    "        KL_divergence = 0\n",
    "        N = len(D)\n",
    "        for it in range(min(N, self.sample_size)):\n",
    "            if it == 0:\n",
    "                s0, u, _, _ = D[-1]\n",
    "            else:\n",
    "                idx = np.random.randint(0, N)\n",
    "                s0, u, _, _ = D[idx]\n",
    "            # u should be sampled\n",
    "            KL_divergence += self.log_prob(s0, u) - q_func(s0, u)\n",
    "        return KL_divergence\n",
    "    \n",
    "    def update(self, D, q_func):\n",
    "        losses = []\n",
    "        for _ in range(self.n_epochs):\n",
    "            loss = self.loss(D, q_func)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            self.model.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            self.optimizer.step()\n",
    "        return losses\n",
    "    \n",
    "    def get_control(self, state):\n",
    "        u_star = self.model(state)\n",
    "        xi = np.random.normal()\n",
    "        u = u_star + xi * self.stdev\n",
    "        return u, u_star\n",
    "    \n",
    "    def grad_phi(self, state, control, q_func):\n",
    "        params = self.params\n",
    "        grad_phi_log_pi = jax.grad(self.log_pi)(params, state, control)\n",
    "        grad_u = self.grad_u_log_pi(state, control)\n",
    "        #state_tensor = torch.tensor([state]).to(torch.float32)\n",
    "        #control_tensor = torch.tensor([control], requires_grad=True).to(torch.float32)\n",
    "        q_value = q_func(state, control)\n",
    "        q_value.backward()\n",
    "        grad_Q = control.grad\n",
    "        #grad_Q = jax.grad(q_func, argnums=1)(state, control)\n",
    "        return grad_phi_log_pi + (grad_u - grad_Q)*state\n",
    "        \n",
    "    def grad_u_log_pi(self, state, control):\n",
    "        mu = self.model(state)\n",
    "        grad = -(control - mu) / (self.stdev**2)\n",
    "        return grad\n",
    "\n",
    "    def log_pi(self, params, state, control):\n",
    "        mu = jnp.dot(stop_gradient(params), state)\n",
    "        prob = -.5 * ((stop_gradient(control) - mu) / self.stdev)**2 - jnp.log(self.stdev) + jnp.log(2*jnp.pi)/2\n",
    "        return prob[0]\n",
    "\n",
    "    def log_prob(self, state, control):\n",
    "        mu = self.predict(state)\n",
    "        return -.5 * ((control - mu) / self.stdev)**2 - jnp.log(self.stdev) + jnp.log(2*jnp.pi)/2\n",
    "\n",
    "\n",
    "key = jrandom.PRNGKey(0)\n",
    "T = 100\n",
    "x0 = jnp.array([2, 0])\n",
    "SDI = StochasticDoubleIntegrator(x0)\n",
    "\n",
    "dim_q = (3, 32, 1)\n",
    "dim_v = (2, 32, 1)\n",
    "dim_pi = (2, 1)\n",
    "SQF = SoftQFunction(dim_q)\n",
    "SVF = SoftValueFunction(dim_v)\n",
    "PI = SoftPolicyFunction(dim_pi)\n",
    "\n",
    "time_horizon = np.arange(0, T, SDI.dt)\n",
    "D = []\n",
    "\n",
    "for t in time_horizon:\n",
    "    s0_estimate = SDI.observe(key)\n",
    "    tensor_s0 = torch.tensor([s0_estimate]).to(torch.float32)\n",
    "    u, _ = PI.get_control(tensor_s0)\n",
    "    x, cost, done = SDI.update(key, u.detach().numpy()[0], info=True)\n",
    "    s1_estimate = SDI.observe(key)\n",
    "    tensor_s1 = torch.tensor([s1_estimate]).to(torch.float32)\n",
    "    D.append((tensor_s0, u, float(-cost), tensor_s1))\n",
    "\n",
    "    SQF.update(D, SVF.model)\n",
    "    SVF.update(D, SQF.get_output, PI.log_prob)\n",
    "    PI.update(s0_estimate, u, SQF.get_output)\n",
    "\n",
    "    # step\n",
    "    key, subkey = jrandom.split(key)\n",
    "\n",
    "    if done:\n",
    "        x0 = jrandom.normal(key, (2,))*2\n",
    "        SDI.reset(x0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftActorCritic:\n",
    "    def __init__(self, key, dim_q, dim_v, dim_pi):\n",
    "        self.SQF = SoftQFunction(dim_q)\n",
    "        self.SVF = SoftValueFunction(dim_v)\n",
    "        self.PI = SoftPolicyFunction(key, dim_pi)\n",
    "        self.buffer = list()\n",
    "        #self.tracker = Tracker(['state0', 'state1', 'control', 'cost', 'V_value', 'V_loss', \n",
    "        #                            'Q_value', 'Q_loss', 'policy_angle', 'policy_force'])\n",
    "    \n",
    "    def update(self, s0, u, tracking=True):\n",
    "        v_value = self.SVF.get_value(s0)\n",
    "        q_value = self.SQF.get_value(s0, u)\n",
    "        v_loss = self.SVF.update(self.buffer, self.SQF.get_output, self.PI.log_prob)\n",
    "        q_loss = self.SQF.update(self.buffer, self.SVF)\n",
    "\n",
    "        self.PI.update(s0, u, self.SQF.predict)\n",
    "\n",
    "        #if tracking:\n",
    "        #    control_angle = jnp.arctan2(self.PI.params[0,0], self.PI.params[0,1])\n",
    "        #    control_force = jnp.linalg.norm(self.PI.params)\n",
    "        #    self.tracker.add([s0[0], s0[1], u, None, v_value, v_loss, q_value, q_loss,\n",
    "        #                            control_angle, control_force])\n",
    "    \n",
    "    def get_control(self, state):\n",
    "        return self.PI.get_control(state)\n",
    "    \n",
    "    def add_to_buffer(self, transition):\n",
    "        self.buffer.append(transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4414/2586416923.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mSAC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms0_estimate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1_estimate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mSAC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms0_estimate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4414/2313217159.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, s0, u, tracking)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mv_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mq_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSQF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSQF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4414/3569274443.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "key = jrandom.PRNGKey(0)\n",
    "\n",
    "dim_q = (3, 32, 1)\n",
    "dim_v = (2, 32, 1)\n",
    "dim_pi = (2, 1)\n",
    "\n",
    "SAC = SoftActorCritic(key, dim_q, dim_v, dim_pi)\n",
    "\n",
    "key = jrandom.PRNGKey(1)\n",
    "key, subkey = jrandom.split(key)\n",
    "\n",
    "T = 500\n",
    "n_obs = 2\n",
    "n_ctrl = 1\n",
    "\n",
    "\n",
    "# Initiate system\n",
    "x0 = jnp.array([2, 0])\n",
    "SDI = StochasticDoubleIntegrator(x0, boundary=5)\n",
    "\n",
    "time_horizon = np.arange(0, T, SDI.dt)\n",
    "\n",
    "for _ in time_horizon:\n",
    "    s0_estimate = SDI.observe(key)\n",
    "    u, _ = SAC.get_control(s0_estimate)\n",
    "    _, cost, done = SDI.update(key, u, info=True)\n",
    "    s1_estimate = SDI.observe(subkey)\n",
    "    SAC.add_to_buffer((s0_estimate, u, -cost, s1_estimate))\n",
    "\n",
    "    SAC.update(s0_estimate, u)\n",
    "\n",
    "    # step\n",
    "    key, subkey = jrandom.split(key)\n",
    "\n",
    "    if done:\n",
    "        x0 = jrandom.normal(key, (2,))*2\n",
    "        SDI.reset(x0)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.15 ('ML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c7394171d7d729d420cd382c214c1b5e828b79606e2dcfa15a106dc0d624623"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
