{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aim\n",
    "\n",
    "Policy gradient methods aim to optimize the policy parameters w.r.t. the expected cost-to-go by gradient descent.\n",
    "\n",
    "As is the case in both Reinforcement Learning and optimal control as a whole, the aim is to find a policy $\\theta$ that minimizes the optimal cost to go $J^*$.\n",
    "\\begin{equation}\n",
    "J^*(s, \\theta, t) = C(s, \\theta, t) + \\gamma \\langle J^*(s, \\theta, t+1)\\rangle\n",
    "\\end{equation}\n",
    "The marginal cost is described by the function $C(\\cdot)$ with state $s$ and time $t$. Discount value $\\gamma$ dimishishes the value of future cost. In order to find the policy with the optimal cost-to-go, the policy parameterization is updated according to the gradient update rule\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta J^*_{\\theta_t}\n",
    "\\end{equation}\n",
    "\n",
    "The challenge in both robotics and control is to find the gradient $\\nabla_\\theta J^*_{\\theta_t}$. Optimal methods rely on knowledge of the system to establish this gradient. However, autonomous and adaptive systems need the ability to establish this gradient without knowing a full model of the system. The resulting challenge is to estimate the policy gradient from data generated during the execution af the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finite-difference Methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "from src.systems.linear import StochasticDoubleIntegrator\n",
    "\n",
    "x0 = jnp.array([2, 0])\n",
    "SDI = StochasticDoubleIntegrator(x0)\n",
    "\n",
    "n_inputs = 2\n",
    "n_ctrl = 1\n",
    "n_steps = 10\n",
    "\n",
    "key = jrandom.PRNGKey(0)\n",
    "\n",
    "params = jrandom.normal(key, (n_inputs, 1))\n",
    "\n",
    "for i in range(n_steps - 1):\n",
    "    y0 = SDI.observe(key)\n",
    "    u_star = jnp.dot(params, y0)\n",
    "\n",
    "    # state update\n",
    "    SDI.update(u_star)\n",
    "\n",
    "    # learning step\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
